{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the json file to csv\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Reading JSON data from a file\n",
    "with open(\"/home/hema/english.json\", 'r', encoding='utf-8') as file:\n",
    "    json_data = file.readlines()\n",
    "    # remove the extra data causing issues\n",
    "    json_data = \"[\" + ','.join([line.strip() for line in json_data if 'text' in line]) + \"]\"\n",
    "\n",
    "# Converting JSON data to a pandas DataFrame\n",
    "df = pd.read_json(json_data)\n",
    "\n",
    "# Writing DataFrame to a CSV file\n",
    "df.to_csv(\"english.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "702101"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = ((df['gender']=='f')|(df['gender']=='m'))\n",
    "df = df\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'gender', 'age', 'country', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the data\n",
    "\n",
    "# Dropping the required columns\n",
    "df = df.drop(columns=[\"did\", \"uid\", \"date\"])\n",
    "\n",
    "# Writing the modified DataFrame to a CSV file\n",
    "df.to_csv(\"preprocessed_data.csv\", index=False)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "# 80% for training, 10% for testing and 10% for development.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset into a pandas DataFrame\n",
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "\n",
    "# Split your data into train, test, and development sets\n",
    "x_train, x_test_dev, y_train, y_test_dev = train_test_split(df.drop(['text', 'gender', 'age', 'country'], axis=1), df[['text', 'gender', 'age', 'country']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the test+dev data into test and development sets\n",
    "x_test, x_dev, y_test, y_dev = train_test_split(x_test_dev, x_test_dev, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# print(x_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train, test_dev = train_test_split(df, test_size=0.2, random_state=42)\n",
    "dev, test = train_test_split(test_dev, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save train, validation, and test sets as TSV files\n",
    "train.to_csv('train.tsv', sep='\\t', index=False)\n",
    "dev.to_csv('dev.tsv', sep='\\t', index=False)\n",
    "test.to_csv('test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(data_path, test=False):\n",
    "    x = []\n",
    "    y = []\n",
    "    genders = []  # new list to store gender information\n",
    "\n",
    "    with open(data_path, encoding='utf8') as dfile:\n",
    "        cols = dfile.readline().strip().split('\\t')\n",
    "\n",
    "        text_idx = 0\n",
    "        label_idx = 4\n",
    "        gender_idx = 1  # index of the column containing gender information\n",
    "\n",
    "        next(dfile)  # skip the header line\n",
    "\n",
    "        for line in dfile:\n",
    "            line = line.strip().split('\\t')\n",
    "\n",
    "            if len(line) < 5:  # check if line has enough elements\n",
    "                continue\n",
    "\n",
    "            x.append(line[text_idx])\n",
    "            y.append(int(round(float(line[label_idx]))))\n",
    "            genders.append(line[gender_idx])  # extract gender information\n",
    "\n",
    "    return x, y, genders  # return x, y, and genders\n",
    "\n",
    "\n",
    "train_x, train_y, train_genders = load_and_preprocess('train.tsv') # training set\n",
    "dev_x, dev_y, dev_genders = load_and_preprocess('dev.tsv') # development set\n",
    "test_x, test_y, test_genders = load_and_preprocess('test.tsv', test=True) # test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Step 1: Data preparation\n",
    "train_reviews = train_x  # List of product reviews for training\n",
    "train_labels = train_y  # List of corresponding sentiment labels for training\n",
    "\n",
    "eval_reviews = test_x  # List of product reviews for evaluation\n",
    "eval_labels = test_y  # List of corresponding sentiment labels for evaluation\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "train_encodings = tokenizer(train_reviews, truncation=True, padding=True, max_length=150)\n",
    "eval_encodings = tokenizer(eval_reviews, truncation=True, padding=True, max_length=150)\n",
    "\n",
    "# Step 3: Creating input sequences\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                                              torch.tensor(train_encodings['attention_mask']),\n",
    "                                              torch.tensor(train_labels))\n",
    "\n",
    "eval_dataset = torch.utils.data.TensorDataset(torch.tensor(eval_encodings['input_ids']),\n",
    "                                             torch.tensor(eval_encodings['attention_mask']),\n",
    "                                             torch.tensor(eval_labels))\n",
    "\n",
    "# Step 4: Training setup\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "num_labels=2) # 2 for binary sentiment classification\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "\n",
    "# Step 5: Training process\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "total_train_batches = len(train_dataloader)\n",
    "total_epochs = 1  # Change the number of epochs if desired\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_samples = 0\n",
    "    correct_train_predictions = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs.logits, 1)\n",
    "        correct_train_predictions += (predicted_labels == labels).sum().item()\n",
    "\n",
    "        total_train_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress update\n",
    "        progress = (batch_idx + 1) / total_train_batches * 100\n",
    "        print(f\"Epoch: {epoch + 1} | Batch: {batch_idx + 1}/{total_train_batches} | Progress: {progress:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_accuracy = 0\n",
    "    total_eval_samples = 0\n",
    "    correct_eval_predictions = 0\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        eval_accuracy += torch.sum(predictions == labels).item()\n",
    "        total_eval_samples += labels.size(0)\n",
    "        correct_eval_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / total_train_batches\n",
    "    avg_train_accuracy = correct_train_predictions / total_train_samples\n",
    "    avg_eval_accuracy = correct_eval_predictions / total_eval_samples\n",
    "\n",
    "    # Print information to monitor the training process\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs // 60\n",
    "    secs = secs % 60\n",
    "    print(f\"Epoch: {epoch + 1} | Time: {mins} minutes, {secs} seconds\")\n",
    "    print(f\"\\tLoss: {avg_train_loss:.4f} (Train) | Accuracy: {avg_train_accuracy*100:.1f}% (Train)\")\n",
    "    print(f\"\\tAccuracy: {avg_eval_accuracy*100:.1f}% (Eval)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9810\n",
      "Accuracy: 98.08%\n",
      "Recall: 0.9808\n",
      "ROC-AUC score: 0.9221\n",
      "F1 score: 0.9809\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85      4342\n",
      "           1       0.99      0.99      0.99     65815\n",
      "\n",
      "    accuracy                           0.98     70157\n",
      "   macro avg       0.91      0.92      0.92     70157\n",
      "weighted avg       0.98      0.98      0.98     70157\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3713   629]\n",
      " [  721 65094]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "\n",
    "eval_predictions = []\n",
    "eval_labels = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    eval_predictions.extend(predictions.cpu().tolist())\n",
    "    eval_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "classification_report_dict = classification_report(eval_labels, eval_predictions, digits=4, output_dict=True)\n",
    "precision = classification_report_dict[\"weighted avg\"][\"precision\"]\n",
    "recall = classification_report_dict[\"weighted avg\"][\"recall\"]\n",
    "f1 = classification_report_dict[\"weighted avg\"][\"f1-score\"]\n",
    "accuracy = (sum(eval_labels[i] == eval_predictions[i] for i in range(len(eval_labels))) / len(eval_labels)) * 100\n",
    "roc_auc = roc_auc_score(eval_labels, eval_predictions)\n",
    "\n",
    "print(f\"Precision: {float(precision):.4f}\")\n",
    "print(f\"Accuracy: {float(accuracy):.2f}%\")\n",
    "print(f\"Recall: {float(recall):.4f}\")\n",
    "print(f\"ROC-AUC score: {float(roc_auc):.4f}\")\n",
    "print(f\"F1 score: {float(f1):.4f}\")\n",
    "print()\n",
    "\n",
    "# Additional print statements for detailed information\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(eval_labels, eval_predictions))\n",
    "print()\n",
    "\n",
    "# this confusion matrix is for both the genders in the test data\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(eval_labels, eval_predictions))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male data length: 25077\n",
      "Female data length: 16668\n"
     ]
    }
   ],
   "source": [
    "male_x_reviews = []\n",
    "male_y_labels = []\n",
    "female_x_reviews = []\n",
    "female_y_labels = []\n",
    "\n",
    "# Loop 1: Separate male data\n",
    "for i in range(len(test_x)):\n",
    "    if test_genders[i] == \"m\":\n",
    "        male_x_reviews.append(test_x[i])\n",
    "        male_y_labels.append(test_y[i])\n",
    "\n",
    "# Loop 2: Separate female data\n",
    "for i in range(len(test_x)):\n",
    "    if test_genders[i] == \"f\":\n",
    "        female_x_reviews.append(test_x[i])\n",
    "        female_y_labels.append(test_y[i])\n",
    "\n",
    "# Loop 3: Print data lengths\n",
    "male_data_length = len(male_x_reviews)\n",
    "female_data_length = len(female_x_reviews)\n",
    "print(\"Male data length:\", male_data_length)\n",
    "print(\"Female data length:\", female_data_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hema/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Time: 32 minutes, 31 seconds\n",
      "\tLoss: 0.0096 (Train) | Accuracy: 97.6% (Train)\n",
      "\tAccuracy: 93.4% (Eval)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "eval_reviews = male_x_reviews  # List of product reviews for evaluation\n",
    "eval_labels = male_y_labels  # List of corresponding sentiment labels for evaluation\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "eval_encodings = tokenizer(eval_reviews, truncation=True, padding=True, max_length=150)\n",
    "\n",
    "# Step 3: Creating input sequences\n",
    "\n",
    "eval_dataset = torch.utils.data.TensorDataset(torch.tensor(eval_encodings['input_ids']),\n",
    "                                             torch.tensor(eval_encodings['attention_mask']),\n",
    "                                             torch.tensor(eval_labels))\n",
    "\n",
    "# Step 4: Training setup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "num_labels=2) # 2 for binary sentiment classification\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Training process\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "eval_accuracy = 0\n",
    "total_eval_samples = 0\n",
    "correct_eval_predictions = 0\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    eval_accuracy += torch.sum(predictions == labels).item()\n",
    "    total_eval_samples += labels.size(0)\n",
    "    correct_eval_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "avg_train_loss = total_train_loss / total_train_batches\n",
    "avg_train_accuracy = correct_train_predictions / total_train_samples\n",
    "avg_eval_accuracy = correct_eval_predictions / total_eval_samples\n",
    "\n",
    "# Print information to monitor the training process\n",
    "secs = int(time.time() - start_time)\n",
    "mins = secs // 60\n",
    "secs = secs % 60\n",
    "print(f\"Epoch: {epoch + 1} | Time: {mins} minutes, {secs} seconds\")\n",
    "print(f\"\\tLoss: {avg_train_loss:.4f} (Train) | Accuracy: {avg_train_accuracy*100:.1f}% (Train)\")\n",
    "print(f\"\\tAccuracy: {avg_eval_accuracy*100:.1f}% (Eval)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8876\n",
      "Accuracy: 93.43%\n",
      "Recall: 0.9343\n",
      "ROC-AUC score: 0.5007\n",
      "F1 score: 0.9031\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.00      0.00      1640\n",
      "           1       0.93      1.00      0.97     23437\n",
      "\n",
      "    accuracy                           0.93     25077\n",
      "   macro avg       0.57      0.50      0.48     25077\n",
      "weighted avg       0.89      0.93      0.90     25077\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[    3  1637]\n",
      " [   11 23426]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "\n",
    "eval_predictions = []\n",
    "eval_labels = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    eval_predictions.extend(predictions.cpu().tolist())\n",
    "    eval_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "classification_report_dict = classification_report(eval_labels, eval_predictions, digits=4, output_dict=True)\n",
    "precision = classification_report_dict[\"weighted avg\"][\"precision\"]\n",
    "recall = classification_report_dict[\"weighted avg\"][\"recall\"]\n",
    "f1 = classification_report_dict[\"weighted avg\"][\"f1-score\"]\n",
    "accuracy = (sum(eval_labels[i] == eval_predictions[i] for i in range(len(eval_labels))) / len(eval_labels)) * 100\n",
    "roc_auc = roc_auc_score(eval_labels, eval_predictions)\n",
    "\n",
    "print(f\"Precision: {float(precision):.4f}\")\n",
    "print(f\"Accuracy: {float(accuracy):.2f}%\")\n",
    "print(f\"Recall: {float(recall):.4f}\")\n",
    "print(f\"ROC-AUC score: {float(roc_auc):.4f}\")\n",
    "print(f\"F1 score: {float(f1):.4f}\")\n",
    "print()\n",
    "\n",
    "# Additional print statements for detailed information\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(eval_labels, eval_predictions))\n",
    "print()\n",
    "\n",
    "# this confusion matrix is for both the genders in the test data\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(eval_labels, eval_predictions))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hema/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Time: 37 minutes, 48 seconds\n",
      "\tLoss: 0.0096 (Train) | Accuracy: 97.6% (Train)\n",
      "\tAccuracy: 85.0% (Eval)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "\n",
    "eval_reviews = female_x_reviews  # List of product reviews for evaluation\n",
    "eval_labels = female_y_labels  # List of corresponding sentiment labels for evaluation\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "eval_encodings = tokenizer(eval_reviews, truncation=True, padding=True, max_length=150)\n",
    "\n",
    "# Step 3: Creating input sequences\n",
    "\n",
    "eval_dataset = torch.utils.data.TensorDataset(torch.tensor(eval_encodings['input_ids']),\n",
    "                                             torch.tensor(eval_encodings['attention_mask']),\n",
    "                                             torch.tensor(eval_labels))\n",
    "\n",
    "# Step 4: Training setup\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "num_labels=2) # 2 for binary sentiment classification\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "\n",
    "# Step 5: Training process\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "eval_accuracy = 0\n",
    "total_eval_samples = 0\n",
    "correct_eval_predictions = 0\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    eval_accuracy += torch.sum(predictions == labels).item()\n",
    "    total_eval_samples += labels.size(0)\n",
    "    correct_eval_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "avg_train_loss = total_train_loss / total_train_batches\n",
    "avg_train_accuracy = correct_train_predictions / total_train_samples\n",
    "avg_eval_accuracy = correct_eval_predictions / total_eval_samples\n",
    "\n",
    "# Print information to monitor the training process\n",
    "secs = int(time.time() - start_time)\n",
    "mins = secs // 60\n",
    "secs = secs % 60\n",
    "print(f\"Epoch: {epoch + 1} | Time: {mins} minutes, {secs} seconds\")\n",
    "print(f\"\\tLoss: {avg_train_loss:.4f} (Train) | Accuracy: {avg_train_accuracy*100:.1f}% (Train)\")\n",
    "print(f\"\\tAccuracy: {avg_eval_accuracy*100:.1f}% (Eval)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8939\n",
      "Accuracy: 85.00%\n",
      "Recall: 0.8500\n",
      "ROC-AUC score: 0.4681\n",
      "F1 score: 0.8712\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.04      0.03       889\n",
      "           1       0.94      0.90      0.92     15779\n",
      "\n",
      "    accuracy                           0.85     16668\n",
      "   macro avg       0.48      0.47      0.47     16668\n",
      "weighted avg       0.89      0.85      0.87     16668\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   36   853]\n",
      " [ 1647 14132]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "\n",
    "eval_predictions = []\n",
    "eval_labels = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    eval_predictions.extend(predictions.cpu().tolist())\n",
    "    eval_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "classification_report_dict = classification_report(eval_labels, eval_predictions, digits=4, output_dict=True)\n",
    "precision = classification_report_dict[\"weighted avg\"][\"precision\"]\n",
    "recall = classification_report_dict[\"weighted avg\"][\"recall\"]\n",
    "f1 = classification_report_dict[\"weighted avg\"][\"f1-score\"]\n",
    "accuracy = (sum(eval_labels[i] == eval_predictions[i] for i in range(len(eval_labels))) / len(eval_labels)) * 100\n",
    "roc_auc = roc_auc_score(eval_labels, eval_predictions)\n",
    "\n",
    "print(f\"Precision: {float(precision):.4f}\")\n",
    "print(f\"Accuracy: {float(accuracy):.2f}%\")\n",
    "print(f\"Recall: {float(recall):.4f}\")\n",
    "print(f\"ROC-AUC score: {float(roc_auc):.4f}\")\n",
    "print(f\"F1 score: {float(f1):.4f}\")\n",
    "print()\n",
    "\n",
    "# Additional print statements for detailed information\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(eval_labels, eval_predictions))\n",
    "print()\n",
    "\n",
    "# this confusion matrix is for both the genders in the test data\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(eval_labels, eval_predictions))\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
